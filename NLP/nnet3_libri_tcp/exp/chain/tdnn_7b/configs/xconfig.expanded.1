# This file was created by the command:
# steps/nnet3/xconfig_to_configs.py --xconfig-file exp/chain_cleaned/tdnn_1b_sp/configs/network.xconfig --config-dir exp/chain_cleaned/tdnn_1b_sp/configs/
#It contains the same content as ./xconfig but it was parsed and
#default config values were set.
# See also ./xconfig.expanded.2

input name=ivector dim=100
input name=input dim=40
fixed-affine-layer name=lda affine-transform-file=exp/chain_cleaned/tdnn_1b_sp/configs/lda.mat delay=0 dim=220 input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) write-init-config=True
relu-batchnorm-layer name=tdnn1 add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=[-1] l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=1.0
relu-batchnorm-layer name=tdnn2 add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=Append(-1,0,1,2) l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=1.0
relu-batchnorm-layer name=tdnn3 add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=Append(-3,0,3) l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=1.0
relu-batchnorm-layer name=tdnn4 add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=Append(-3,0,3) l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=1.0
relu-batchnorm-layer name=tdnn5 add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=Append(-3,0,3) l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=1.0
relu-batchnorm-layer name=tdnn6 add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=Append(-6,-3,0) l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=1.0
relu-batchnorm-layer name=prefinal-chain add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=[-1] l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=0.5
output-layer name=output bias-stddev=0.0 dim=5176 include-log-softmax=False input=[-1] l2-regularize=0.0 learning-rate-factor=1.0 max-change=1.5 ng-affine-options= objective-type=linear output-delay=0 param-stddev=0.0 presoftmax-scale-file=
relu-batchnorm-layer name=prefinal-xent add-log-stddev=False bias-stddev= dim=725 dropout-proportion=0.5 input=tdnn6 l2-regularize= learning-rate-factor= max-change=0.75 ng-affine-options= self-repair-scale=1e-05 target-rms=0.5
output-layer name=output-xent bias-stddev=0.0 dim=5176 include-log-softmax=True input=[-1] l2-regularize=0.0 learning-rate-factor=5.0 max-change=1.5 ng-affine-options= objective-type=linear output-delay=0 param-stddev=0.0 presoftmax-scale-file=
