# This file was created by the command:
# steps/nnet3/xconfig_to_configs.py --xconfig-file exp/chain_cleaned/tdnn_1b_sp/configs/network.xconfig --config-dir exp/chain_cleaned/tdnn_1b_sp/configs/
# It is a copy of the source from which the config files in # this directory were generated.

input dim=100 name=ivector
input dim=40 name=input

# please note that it is important to have input layer with the name=input
# as the layer immediately preceding the fixed-affine-layer to enable
# the use of short notation for the descriptor
fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=exp/chain_cleaned/tdnn_1b_sp/configs/lda.mat

# the first splicing is moved before the lda layer, so no splicing here
relu-batchnorm-layer name=tdnn1 dim=725
relu-batchnorm-layer name=tdnn2 dim=725 input=Append(-1,0,1,2)
relu-batchnorm-layer name=tdnn3 dim=725 input=Append(-3,0,3)
relu-batchnorm-layer name=tdnn4 dim=725 input=Append(-3,0,3)
relu-batchnorm-layer name=tdnn5 dim=725 input=Append(-3,0,3)
relu-batchnorm-layer name=tdnn6 dim=725 input=Append(-6,-3,0)

## adding the layers for chain branch
relu-batchnorm-layer name=prefinal-chain dim=725 target-rms=0.5
output-layer name=output include-log-softmax=false dim=5176 max-change=1.5

# adding the layers for xent branch
# This block prints the configs for a separate output that will be
# trained with a cross-entropy objective in the 'chain' models... this
# has the effect of regularizing the hidden parts of the model.  we use
# 0.5 / args.xent_regularize as the learning rate factor- the factor of
# 0.5 / args.xent_regularize is suitable as it means the xent
# final-layer learns at a rate independent of the regularization
# constant; and the 0.5 was tuned so as to make the relative progress
# similar in the xent and regular final layers.
relu-batchnorm-layer name=prefinal-xent input=tdnn6 dim=725 target-rms=0.5
output-layer name=output-xent dim=5176 learning-rate-factor=5.0 max-change=1.5
